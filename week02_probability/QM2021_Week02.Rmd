---
title: "QM 2021 Week 2:<br>Probability and the Central Limit Theorem"
author: "Oliver Rittmann<br>Viktoriia Semenova<br>David Grundmanns"
date: "September 16 | 20 | 21, 2021"
output:
  html_document:
    toc: yes
    toc_float: yes
    smooth_scroll: yes
    css: css/lab.css
  pdf_document:
    toc: yes
---

------------------------------------------------------------------------

## Today we will learn

0.  How to work with **RMarkdown**.

1.  How to look at **Probability Distributions in R**.

    1.  Discrete Probability Distributions.
    2.  Continuous Probability Distributions.

2.  What the **Central Limit Theorem** is.

------------------------------------------------------------------------

## RMarkdown revisited

You have already started getting yourself acquainted with `R Markdown` while working on first assignment in `R Markdown`. Now it's time to learn a bit more about it.

This is an R Markdown document. It has the file extension `.Rmd`. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** or **Preview** button a document will be generated that includes both content, as well as the output of any embedded R code chunks within the document. A R code chunk is anything that goes between ```` ```{r} ... ``` ````.

You can embed a R code chunk by typing it out or with a shortcut in RStudio (Mac: `Cmd+Alt+I`, Windows/Linux: `Ctrl+Alt+I`).

Here is a code chunk for you:

```{r setup}
# The first line sets an option for the final document that can be produced from
# the .Rmd file. Don't worry about it.
knitr::opts_chunk$set(
  echo = TRUE, # show results
  collapse = TRUE # not interrupt chunks
)

# The next bit (lines 50-69) is quite powerful and useful.
# First you define which packages you need for your analysis and assign it to
# the p_needed object.
p_needed <-
  c("viridis","ggplot2")

# Now you check which packages are already installed on your computer.
# The function installed.packages() returns a vector with all the installed
# packages.
packages <- rownames(installed.packages())
# Then you check which of the packages you need are not installed on your
# computer yet. Essentially you compare the vector p_needed with the vector
# packages. The result of this comparison is assigned to p_to_install.
p_to_install <- p_needed[!(p_needed %in% packages)]
# If at least one element is in p_to_install you then install those missing
# packages.
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
# Now that all packages are installed on the computer, you can load them for
# this project. Additionally the expression returns whether the packages were
# successfully loaded.
sapply(p_needed, require, character.only = TRUE)
```

### Chunk Options

Chunk output in your final document can be customized with knitr options, arguments set in the `{}` of a chunk header. Here are some useful arguments:

-   `include = FALSE` prevents code and results from appearing in the finished file. R Markdown still runs the code in the chunk, and the results can be used by other chunks.
-   `echo = FALSE` prevents code, but not the results from appearing in the finished file. This is a useful way to embed figures.
-   `message = FALSE` prevents messages that are generated by code from appearing in the finished file.
-   `warning = FALSE` prevents warnings that are generated by code from appearing in the finished.
-   `fig.cap = "..."` adds a caption to graphical results.

See the [R Markdown Reference Guide](https://rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf?_ga=2.169319849.94966807.1602002830-1180689241.1600075670) for a complete list of knitr chunk options.

### Text formatting options

Markdown also provides an easy way to format the text in your document. Here are some basics:

-   *This text is italic*
-   **This text is bold**
-   `This is code`
-   [This is a link to the R Markdown homepage](https://rmarkdown.rstudio.com)

For more formatting options, we recommend the [R Markdown Cheat Sheet](https://rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf?_ga=2.195559076.94966807.1602002830-1180689241.1600075670).

------------------------------------------------------------------------

## Probability Distributions

All the probability functions are defined in R already. To call them you have to combine a prefix and the distribution name.

The prefixes you can use are d, p, q and r. When to use which prefix?

1.  Use **d** when you know x and want to know the density at that point.

    -   The result is a point of the probability mass/density function.
    -   If you put in a range of x values you will get the **PDF/PMF** of the distribution.

2.  Use **p** when you know q and want to know the probability up to that point.

    -   The result is a point on the cumulative distribution function or the area under the probability density function.
    -   If you put in a range of values you will get the **CDF** of the distribution.

3.  Use **q** when you know the area p and want to know the x value

    -   This is exactly the opposite to p.

4.  Use **r** if you want random numbers drawn from a distribution.

    -   The result is a sample from a distribution.

5.  The distribution names are:

    -   `binom` for the Binomial distribution
    -   `norm` for the Normal distribution
    -   `unif` for the Uniform distribution

For each of the distributions you need to specify different parameters!

------------------------------------------------------------------------

### Discrete Probability Distributions

Let's start with Discrete Probability Distributions. Can you think of examples of Discrete Probability Distributions?

-   Coin flip (multiple times)
-   Number of votes obtained
-   Number of members in a household
-   Gender of a random person

#### The Bernoulli Distribution

The Bernoulli distribution is the simplest Discrete Probability Distribution. The Bernoulli distribution is a special case of the binomial distribution. It is a Binomial distribution with size 1. Thus, it is ideal for modelling **one-time** yes/no (success/failure) events.

```{r Discrete Probability Distributions I}
dbinom(x = 1, size = 1, prob = 0.8)  # What does the result tell us?  

dbinom(x = 0, size = 1, prob = 0.8)  # What does the result tell us?
```

-   `x` = number of successes (e.g. pass an exam)
-   `size` = number of trials (1 coin flip)
-   `prop` = probability of 'success' (e.g. probability of passing an exam)

The best example is **one** coin flip (or one voter voting yes/no, a person being either a woman/man). Let's simulate one simple coin toss:

```{r Discrete Probability Distributions II}
rbinom(n = 1, size = 1, prob = 0.5)
```

Ok, we could have done this with a real coin. Let's toss one coin 1000 times. Any volunteers?

Luckily, R can simulate this quicker.

```{r Discrete Probability Distributions III}
trial1 <- rbinom(n = 1000, size = 1, prob = 0.5)
```

-   `n` = number of observations (i.e. how many coin flips we want to observe)

And now we want to see how successful we were:

```{r Discrete Probability Distributions IV}
table(trial1)
```

#### The Binominal Distribution

We get a binomial distribution if we -- like in the lecture example -- toss a coin twice. What are the possible outcomes, the so-called sample space?

-   **Sample space**: {HH, HT, TH, TT}

-   What is the probability of observing exactly 0 (1, 2) heads (successes)?

```{r Binomial I}
dbinom(x = c(0, 1, 2),
       size = 2,
       prob = 0.5)
```

-   What is the probability of observing 0 or 1 heads (successes)?

```{r Binomial II}
pbinom(1, 2, 0.5)
```

-   What is the probability of more than 1 heads (successes)?

```{r Binomial III}
1 - pbinom(1, 2, 0.5)

# or:

pbinom(1, 2, 0.5, lower.tail = F)
```

#### Example I: Coin Flips

We talked about probability in a frequentist understanding. Let's check if repeating this experiment 1,000 times gives us the values we would expect.

Out of 1000 flips, you would expect 250 HH, 500 HT TH, and 250 TT.

```{r Example I}
ex_trial <- rbinom(n = 1000, size = 2, prob = 0.5)
table(ex_trial)
```

#### The PMF and CDF of a Binomial {.tabset}

Let's plot the binomial probability mass function to get a better understanding.

##### Base R

We start by creating an x-axis and an empty plot.

```{r}
# First, we need an x-axis
x_values <- c(0:100)

# Now we produce an empty plot.
plot(
  x = x_values,
  ylim = c(0, 0.1), # With ylim you can adjust the limits of the y axis.
  type = "n",       # type "n" produces an empty plot window.
  main = "PMF for Binomial (n = 100, p = 0.5)",
  xlab = "x",
  ylab = "p(x)",
  las = 1,
  frame = F
  )

# Let's add the points.
points(x_values, 
       dbinom(x_values, size = 100, p = 0.5), 
       cex = 0.3, 
       pch = 19, 
       col = viridis(1)
       )

# And those lines we know from the lecture.
segments(x0 = x_values, 
         y0 = 0, 
         x1 = x_values, 
         y1 = dbinom(x_values, size = 100, p = 0.5), 
         col = viridis(1)
         )  
```

> **Question: Why should we not use a line to connect the points?**

$\Rightarrow$ ... because the outcome space is discrete, i.e. there is nothing like 2.5 heads out of a given number of coin flips.

What does the cumulative distribution function of the same Binomial look like?

```{r Binomial CDF}
plot(x = x_values, 
     y = pbinom(x_values, size = 100, p = 0.5),
     type = "p",
     main = "CDF for Binomial (n = 100, p = 0.5)",
     xlab = "x",
     ylab = "Probability",
     bty = "n",
     las = 1,
     pch = 19, 
     cex = 0.3, 
     col = viridis(1),
     frame = F
     )
```

##### ggplot2

`ggplot2` is a very widely used package for visualization in R community. We will be providing you with code using this package in addition to the code with bulit-in functions of R.

First, a few words about `ggplot2`. When working with this package, we think about images as being constructed from layers: the *data*-level, the mapping *aesthetics* level, and the *geom* level. These are the essential, but there are many more:

-   *Data:* your data frame
-   *Aesthetic mapping:* how data (variables) become plot attributes (axes, color, sizes)
-   *Geoms:* geometric representations of data (points, lines, bars)
-   *Scales:* modifying the mapping from data to plot (customizing axes, colors, sizes)
-   *Facets:* sub-panels in the plot
-   *Coordinates:* features of the coordinate system (orientation...)

With `ggplot2`package, it is most effective to work with datasets. So we'll start by putting all of our vectors into a `data.frame` object.

```{r Binomial-PMF-ggplot}
library(ggplot2) 

# create a dataset
binom_data <- data.frame(x = x_values, 
                         density = dbinom(x_values, size = 100, p = 0.5))

# plot 
ggplot( # empty plot for our data
  data = binom_data, # data used for plotting
  mapping = aes(x = x, y = density) # axes
) + 
  geom_point(color = viridis(1)) + # add points
  geom_segment(aes( # add lines 
    x = x, xend = x, # how thick the lines will be
    y = 0, yend = density # how long the lines will be
  ),
  color = viridis(1) # color the lines
  ) +
  theme_minimal() + # change the appearance 
  labs(
    title = "PMF for Binomial (n = 100, p = 0.5)",
    y = "p(x)"
  )  
```

> **Question: Why should we not use a line to connect the points?**

$\Rightarrow$ ... because the outcome space is discrete, i.e. there is nothing like 2.5 heads out of a given number of coin flips.

What does the cumulative distribution function of the same Binomial look like?

```{r Binomial-CDF-ggplot}

# Add new column to our dataset 

binom_data$CDF <- pbinom(x_values, size = 100, p = 0.5) 

ggplot(
  data = binom_data, # data used for plotting
  mapping = aes(x = x, y = CDF) # axes
) +
  geom_point(color = viridis(1)) + # add points
  theme_minimal() + # change the appearance 
  labs(
    title = "CDF for Binomial (n = 100, p = 0.5)",
    y = "P(x)"
  )  
```

#### Example II: Airline Reservations {.tabset}

A flight has 100 available seats, but the airline accepted 120 reservations (overbooking).

The probability of each person showing up is 0.85.

Now the airline wants to know the probability that more than 100 people show up.

-   Remember the binomial process:

    -   n independent trials
    -   only two possible outcomes (show up/don't show up)
    -   success probability remains constant

We want to know:

$Pr(X>100)$.

This is $Pr(X=101)+Pr(X=102) +...Pr(X=120)$ (equal to $1-Pr(X\leq100)$.

a)  What is the probability that more than 100 people show up?

```{r Airline example Ia}
# Answer to a):
1 - pbinom(q = 100, size = 120, p = 0.85)
```

b)  Plot the cumulative distribution function of the problem.

##### Base R

```{r Airline example Ib}
# Answer to b):
passengers  <- 0:120 # for x-axis

plot(x = passengers, 
     y = pbinom(q = passengers, size = 120, p = 0.9),
     bty = "n",
     las = 1,
     pch = 19,
     xlab = "Number of People Showing Up",
     ylab = "Probability",
     main = "Flight Overbooking Probability\nCDF of Binomial (n = 120, p = 0.85)",
     cex = 0.5, 
     col = viridis(1),
     frame = F
     )
```

##### ggplot2

```{r Airline example Ib-ggplot}
# Answer to b):

# create a dataframe 
airline_data <- data.frame(x = 0:120,
                           CDF = pbinom(q = 0:120, size = 120, p = 0.85))
# plot 
ggplot(data = airline_data,  # data used for plotting
       mapping = aes(x = x, y = CDF)) +
  geom_point(color = viridis(1)) + # add points
  theme_minimal() + # change the appearance
  labs(x = "Number of People Showing Up",
       y = "Probability",
       title = "Flight Overbooking Probability\nCDF of Binomial (n = 120, p = 0.85)")  
```

####  {.unnumbered}

c)  What is the probability of more than 100 people showing up if the airline limits overbooking to 110?

```{r Airline example Ic}
# Answer to c):
1 - pbinom(q = 100, size = 110, p = 0.85)
```

------------------------------------------------------------------------

### Continuous Probability Distributions {.tabset}

We will be often looking at normal distribution. The most common normal distribution is the standard normal distribution.

> **What do you know about the standard normal distribution?**

Let's go through those prefixes for the normal distribution again. If you want to know the probability that $x < 1.5$, use $p$:

```{r Continuous Distributions I}
# mean = 0, varaince = 1 by default
pnorm(q = 1.5, mean = 0, sd = 1)
```

Conversely, if you want to know the probability that $x > 1.5$, use:

```{r Continuous Distributions II}
1 - pnorm(q = 1.5, mean = 0, sd = 1)
```

If you want to know at what value of $x$ do 30% of the data lie, use $q$:

```{r Continuous Distributions III}
qnorm(p = 0.3, mean = 0, sd = 1)
```

If you want to generate some random numbers of the normal distribution use $r$:

```{r Continuous Distributions IV}
rnorm(n = 20, mean = 0, sd = 1)
```

It is a good idea to plot those distributions (since `R` can draw better than me):

#### Base R

```{r Continuous Distributions V, fig.show="hold", out.width="50%"}
x_values <- seq(from = -5, to = 5, by = 0.1)

plot(x = x_values, 
     y = dnorm(x_values),
     bty = "n", 
     las = 1,
     type = "l",
     col = viridis(1),
     lwd = 2,
     main = "PDF of N(0, 1)",
     ylab = "f(x)",
     xlab = "x"
     )

plot(x = x_values, 
     y = pnorm(x_values),
     bty = "n", 
     las = 1,
     type = "l",
     col = viridis(1),
     lwd = 2,
     main = "CDF of N(0, 1)",
     ylab = "F(x)",
     xlab = "x"
     )

# What if we have a different mean and sd?
plot(x = x_values, 
     y = dnorm(x_values, mean = 3, sd = 0.1),
     bty = "n", 
     las = 1,
     type = "l",
     col = viridis(1),
     lwd = 2,
     main = "PDF of N(1, 2)",
     ylab = "f(x)",
     xlab = "x"
     )
```

#### ggplot2

```{r Continuous Distributions V-ggplot, fig.show="hold", out.width="50%"}
x_values <- seq(from = -5, to = 5, by = 0.1)

# ggplot can also work without datasets

ggplot() + # this creates an empty plot
  geom_line(aes(x = x_values, y = dnorm(x_values)),
            color = viridis(1)) +
  labs(title = "PDF of N(0, 1)",
       y = "f(x)",
       x = "x") +
  theme_bw()

ggplot() + # this creates an empty plot
  geom_line(aes(x = x_values, y = pnorm(x_values)),
            color = viridis(1)) +
  labs(title = "CDF of N(0, 1)",
       y = "F(x)",
       x = "x") +
  theme_bw()

ggplot() + # this creates an empty plot
  geom_line(aes(x = x_values, y = dnorm(x_values, mean = 3, sd = 0.1)),
            color = viridis(1)) +
  labs(title = "PDF of N(1, 2)",
       y = "f(x)",
       x = "x") +
  theme_bw()

```

###  {.unnumbered}

#### Example III: Normal Approximation of the Binomial {.tabset}

We saw that the binomial distribution can be approximated by the normal. Let's calculate the airplane reservation example using a normal distribution.

First, we have to think about the underlying distribution. Using the central limit theorem, we can translate the binomial process into a normal distribution. 0.85 percent of the people are expected to show up, thus the mean can be calculated by $E(X)=n*p$. The variance can be calculated using $Var(X)=n*p*(1-p)$ (see slide 15).

Let's calculate the mean and the variance of the normal distribution:

```{r Airline Example II}
n <- 120
p <- 0.85
mean <- n * p
sd <- sqrt((n * p * (1 - p)))  # Why do we have to use the square root?
```

Let's solve an example with the normal distribution:

a)  What is the probability that more than 100 people will show up, if we accept 120 reservations?
b)  Plot the normal distribution.
c)  Compare your result to the Binomial process from above.

##### Base R

```{r Airline Example II ctd}
# part a)
1 - pnorm(q = 100, mean = mean, sd = sd)

# part b)
passengers <- seq(from = 0, to = 120, by = 0.01)
densities <- dnorm(passengers, mean = mean, sd = sd)

plot(
  x = passengers,
  y = densities,
  bty = "n",
  las = 1,
  col = viridis(1),
  xlab = "Number of People Showing Up",
  ylab = "Probability Density",
  type = "l", 
  lwd = 2, 
  cex = 2, 
  main = "Flight Overbooking Probability \nPDF of N(mean = 102, var = 15.3)"
)

# part c)
1 - pnorm(q = 100, mean = mean, sd = sd)
1 - pbinom(q = 100, size = n, prob = p)
```

##### ggplot2

```{r Airline Example II ctd-ggplot}
# part a)
1 - pnorm(q = 100, mean = mean, sd = sd)

# part b)
# modify dataset from before 
airline_data$CDF_normal <- dnorm(airline_data$x, mean = mean, sd = sd)

ggplot(airline_data,
       aes(x = x,
           y = CDF_normal)) +
  geom_point(color = viridis(1)) +
  labs(
    title = "Flight Overbooking Probability \nPDF of N(mean = 102, var = 15.3)",
    x = "Number of People Showing Up",
    y = "Probability Density"
  )  +
  theme_bw()

# part c)
1 - pnorm(q = 100, mean = mean, sd = sd)
1 - pbinom(q = 100, size = n, prob = p)
```

------------------------------------------------------------------------

## Central Limit Theorem

Finally, we want to better understand the central limit theorem. Let's generate a random population:

```{r Central Limit Theorem I, fig.show="hold", out.width="50%"}
pop <- rnorm(1000, mean = 32, sd = 5) # we know the "true" values
summary(pop)
var(pop)

hist(pop,
     bty = "n",
     las = 1,
     border = "white",
     col = viridis(1)
     )

plot(density(pop),
     bty = "n",
     las = 1,
     col = viridis(1),
     lwd = 2,
     main = "Density of pop"
     )

sample(pop, 200) # take a random sample of size 200 from the population
```

### CLT Trial 1: Sample Size = 100

Let's take 100 samples with 100 persons in each sample. First, we create an empty vector - `trial1` - of length 100 to store the results:

```{r Central Limit Theorem II}
trial1 <- rep(NA, 100) 
```

Now we have to do some programming. We create a *for loop* counting $i$ from 1 to 100. For each iteration the loop is saving the mean of our sample in the vector `trial1`.

```{r Central Limit Theorem III}
for (i in 1:100){
  trial1[i] <- mean(sample(pop, 100))
}

trial1
```

Let's have a look at our *sampling distribution*:

```{r Central Limit Theorem IV}
hist(trial1,
     bty = "n",
     las = 1,
     border = "white",
     col = viridis(1),
     xlim = c(30, 34),
     main = "Distribution of sample means\n(sample size: n = 100)"
     )

mean(trial1)
var(trial1)
```

### CLT Trial 2: Sample Size = 400

We do it again. But this time we will take 100 samples with 400 people in each sample. It works just as above.

```{r Central Limit Theorem V}
trial2 <- rep(NA, 100)

for (i in 1:100){
  trial2[i] <- mean(sample(pop, 400))
}

hist(trial2,
     bty = "n",
     las = 1,
     border = "white", 
     col = viridis(1),
     xlim = c(30, 34), 
     main = "Distribution of sample means\n(sample size: n = 400)"
     )

mean(trial2)
var(trial2)
```

Does the variance of the sampling distribution behave according to $\frac{sd^2}{n}$?

The original standard deviation value was 5, so $\frac{25}{n}$ should be the variance of our sampling distribution!

```{r Central Limit Theorem VI}
var(pop)/400
```

In the lecture we learned that the sampling distribution of the mean always approaches a normal distribution, regardless of the shape of the original distribution!

You don't believe me? See for yourself.

Income, for example, is usually not really normally distributed. First, we generate some hypothetical income data.

```{r Central Limit Theorem VII}
income <- rgamma(1000, shape = 1.1, scale = 2000) 
summary(income)
var(income)

# Let's have a look.
par(mfrow = c(1, 2)) # two plots side-by-side (alternative to chunk arguments)
hist(income,
     bty = "n",
     las = 1,
     border = "white",
     col = viridis(2)[1]
     )

plot(density(income),
     bty = "n",
     las = 1,
     col = viridis(2)[2],
     lwd = 2,
     main = "Density of income"
     )
```

This is clearly a non-normal distribution. Now we want to get our sampling distribution of the mean again.

### CLT Trial 3: A non-normal population distribution

```{r Central Limit Theorem IX, fig.show="hold", out.width="50%"}
trial3 <- rep(NA, 100)

for (i in 1:100){
  trial3[i] <- mean(sample(income, 100))
}

hist(trial3 ,
     bty = "n",
     las = 1,
     border = "white",
     col = viridis(1),
     xlim = c(1500, 3000),
     main = "Distribution of sample means\n(sample size: n = 100)"
     )

plot(density(trial3),
     bty = "n",
     las = 1,
     lwd = 2,
     col = viridis(1),
     xlim = c(1500, 3000),
     main = "Density of sample means\n(sample size: n = 100)"
     )

mean(trial3)
var(trial3)
```

### CLT Trial 4: A non-normal population distribution II {.tabset}

#### Base R

```{r Central Limit Theorem X, fig.show="hold", out.width="50%"}
trial4 <- rep(NA, 100)

for (i in 1:100){
  trial4[i] <- mean(sample(income, 400)) #this time, we sample 400 people
}

hist(trial4,
     bty = "n",
     las = 1,
     border = "white",
     col = viridis(1),
     xlim = c(1500, 3000),
     main = "Distribution of sample means\n(sample size: n = 400)"
     )

plot(density(trial4),
     bty = "n", 
     las = 1,
     lwd = 2,
     col = viridis(1),
     xlim = c(1500, 3000),
     main = "Density of sample means\n(sample size: n = 400)"
     )

mean(trial4)
var(trial4)
```

#### ggplot2

```{r Central Limit Theorem X-ggplot, fig.show="hold", out.width="50%"}
trial4 <- rep(NA, 100)

for (i in 1:100){
  trial4[i] <- mean(sample(income, 400)) #this time, we sample 400 people
}

ggplot() +
  geom_histogram(aes(x = trial4),
                 boundary = 2200, 
                 binwidth = 20,
                 color = "white",
                 fill = viridis(1)) +
  theme_bw() +
  labs(title = "Distribution of sample means\n(sample size: n = 400)")

ggplot() +
 geom_density(aes(trial4)) +
  labs(title = "Density of sample means\n(sample size: n = 400)",
       y = "F(x)",
       x = "x") +
  theme_bw()

mean(trial4)
var(trial4)
```

------------------------------------------------------------------------

### Set Seeds!

Setting seeds is extremely helpful for the homework and replicability in general. To do so, use the following command: `set.seed()`.

```{r Seeds}
a <- rnorm(5)

b <- rnorm(5)

a == b


set.seed(1234)
a <- rnorm(5)

set.seed(1234)
b <- rnorm(5)

a == b
```

------------------------------------------------------------------------

## Concluding remarks

Next, you find some exercises to check your knowledge.

------------------------------------------------------------------------

## Exercises

### Exercise A: Airline reservations again

Now the airline also has a bigger plane.

The flight has 250 available seats, but the airline accepted 275 reservations (overbooking).

Since this is a long haul flight, the probability of each person showing up is a bit higher at 0.89.

Now the airline wants to know the probability that more than 250 people show up.

-   Remember the binomial process:

    -   n independent trials
    -   only two possible outcomes (show up, don't show up)
    -   success probability remains constant

We want to know:

$Pr(X>250)$.

This is $Pr(X=251)+Pr(X=252) +...Pr(X=275)$ (equal to $1-Pr(X<=250)$.

a)  What is the probability that more than 250 people show up?
b)  Plot the cumulative distribution function of the problem.
c)  What is the probability of more than 250 people showing up if the airline limits overbooking to 260?

```{r Airline Exercise I}

```

### Exercise B: Airline reservations and the Normal Approximation

We saw that the binomial distribution can be approximated by the normal. Let's calculate the airplane reservation example using a normal distribution.

First, we have to think about the underlying distribution. Using the central limit theorem, we can translate the binomial process into a normal distribution. 0.89 percent of the people are expected to show up, thus the mean can be calculated by $E(X)=n*p$. The variance can be calculated using $Var(X)=n*p*(1-p)$ (see slide 15).

Let's calculate the mean and the standard deviation of the normal distribution:

```{r Airline Exercise II}

```

Let's solve an example with the normal distribution:

a)  What is the probability that more than 250 people will show up, if we make 275 reservations?
b)  Plot the normal distribution.
c)  Compare your result to the Binomial process from above.

```{r Airline Exercise II ctd}


```
