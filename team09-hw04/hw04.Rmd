---
title: "Quantitative Methods in Political Science - Homework 4"
author: 
  - "Simran Suresh Bhurat (33%)"
  - "Tobias Fechner (33%)"
  - "Tobias Jochen Sesterhenn (33%)"
date: "Due: October 12, 2021"
output:
  html_document:
    toc: no
  pdf_document:
    toc: yes
---


```{r setup, include=FALSE}
# The first line sets an option for the final document that can be produced from
# the .Rmd file. Don't worry about it.
knitr::opts_chunk$set(echo = TRUE)

# The next bit (lines 22-43) is quite powerful and useful. 
# First you define which packages you need for your analysis and assign it to 
# the p_needed object. 
p_needed <- c(
  "viridis", 
  "stargazer",
  "foreign", # import files
  "dplyr" # for glimpse command
)

# Now you check which packages are already installed on your computer.
# The function installed.packages() returns a vector with all the installed 
# packages.
packages <- rownames(installed.packages())
# Then you check which of the packages you need are not installed on your 
# computer yet. Essentially you compare the vector p_needed with the vector
# packages. The result of this comparison is assigned to p_to_install.
p_to_install <- p_needed[!(p_needed %in% packages)]
# If at least one element is in p_to_install you then install those missing
# packages.
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
# Now that all packages are installed on the computer, you can load them for
# this project. Additionally the expression returns whether the packages were
# successfully loaded.
sapply(p_needed, require, character.only = TRUE)

# This is an option for stargazer tables
# It automatically adapts the output to html or latex,
# depending on whether we want a html or pdf file
stargazer_opt <- ifelse(knitr::is_latex_output(), "latex", "html")
```

**Notes:**

> **Note:** If you do not have any special reason, please do not load additional packages to solve this homework assignment. If you nevertheless do so, please indicate why you think this is necessary and add the package to the `p_needed` vector in the setup chunk above.

> In addition to the `Rmd`, please make sure that in the repo, there is a PDF knitted from your code. The automated check for reproducibility on Github will run only when you include the word "final" into the commit message.

> *Please try to answer the questions with **short** but very **precise** statements. However, do not hide behind seemingly fancy jargon.*

## Part 1: Linear Regression Basics

**1.1 For the following data set, calculate the OLS estimator of slope and intercept *by hand* (not in R, by hand on paper). Show your calculations.**

*Hint: Take a look at slide 14 from the lecture. You can write LaTeX-code for mathematical formulas like this: $0.5 = \frac{1}{2}$.*

|  x  |   y  |
|:--: |:----:|
| 3.2 | 1.8  |
| 6   | 8.5  |
| 4.5 | 4.8  |
| 10  | 11.5 |
| 12  | 14.5 |

Table: Simple Data Set

![Caption for the picture.](./images/Loesung1.png)



**1.2 Then read in the data in R by hand. Write a function that calculates the slope and intercept in `R` and apply it to the data set.** 

*Hint: Create two vectors using the following command:`x <- c(1,2,3,...)`.*

```{r Exercise 1_2}

x <- c(3.2, 6, 4.5, 10, 12)
y <- c(1.8, 8.5 , 4.8, 11.5, 14.5)

calculate_slope <- function(x,y) {
  covariance <- sum((x - mean(x)) * (y - mean(y)))
  variance <- sum((x - mean(x))^2)
  slope <- covariance/variance
  return(slope) 
 
}

calculate_intercept <- function(x,y) {
  covariance <- sum((x - mean(x)) * (y - mean(y)))
  variance <- sum((x - mean(x))^2)
  slope <- covariance/variance
  intercept <- mean(y) - slope * mean(x)
  return(intercept)
 
}
print("calculated_slope: " )
calculate_slope(x,y)


print("calculated_intercept: ")
calculate_intercept(x,y)

```



**1.3 Use the built-in regression functions in `R` and compare the results to your calculations from 1.1 and the results in 1.2.**

```{r Exercise 1_3}

x <- c(3.2, 6, 4.5, 10, 12)
y <- c(1.8, 8.5 , 4.8, 11.5, 14.5)

linear_model <- lm(y ~ x)

# slope and intercept
summary(linear_model)

# Intercept and Slope are found in the red box

```
![Caption for the picture.](./images/Loesung13.png)

## Part 2: Corruption and Wealth

*Political and economic corruption is an annoying problem throughout the world. How does the extent of corruption change with the wealth of countries, if at all?* 

*Transparency International provides estimates of corruption levels in countries of the world based on surveys of business people, risk analysts and the general public. The index ranges between 10 (highly clean) and 0 (highly corrupt). Your task is to assess the substantive impact of wealth on corruption and evaluate the substantive relevance of this effect.*  

*You will need the `corruption.dta` which is in the `raw-data` folder of this repository.*

**2.1 The cross-sectional data set contains the average corruption index between 2000 and 2007 and the (per capita) wealth variables in 2002. How many countries does the data set include? Which countries are the least and most corrupt? Which countries are the least and most wealthy?**
  
```{r Exercise 2_1}


data <- read.dta("raw-data/corruption.dta")
head(data)
glimpse(data) # function from dplyr package
column_names <- colnames(data)
df = data.frame(data)

```


```{r}


# How many countries does the data set include?
print("How many countries does the data set include?")
length(df$cname)
```


```{r}
# Which countries are the least corrupt?

#top_frac(df, n = 5, cpi)
#head(sort(df$cpi,decreasing=TRUE), n = 5)
#head(arrange(df,desc(cpi)), n = 5)

head(df[order(df$cpi, decreasing= T),], n = 5)

```
```{r}
# Which countries are the most corrupt?

head(df[order(df$cpi, decreasing= F),], n = 5)

```


```{r}
# Which countries are the least wealthy?
head(df[order(df$gdp, decreasing= T),], n = 5)
# Which countries are the most wealthy?
head(df[order(df$gdp, decreasing= F),], n = 5)
#subset(df, (Gender == 'Male' & Color == 'Blue'))

```




**2.2 Run a regression of corruption on GDP per capita, with the corruption score as the dependent variable. Make a nicely formatted (!) table from the output. Knit the `.Rmd` file to a `.pdf` file to check whether your table looks good. Write down the regression line equation. What is the substantial meaning of the estimated intercept and slope?**

```{r Exercise2.2 stargazer, message = F, results='asis'}

linear_model_corruption <- lm(df$cpi ~ df$gdp)

# slope and intercept
#summary(linear_model)

stargazer(linear_model_corruption, 
          type = stargazer_opt, #"Text",
          covariate.labels = c("GDP per capita of countries"),
          dep.var.labels = c("Corruption Score of countries"))

```

$ y = {\beta_{0}} + {\beta_{1}}x_{1} $

$ y = -1.272 + 1.329 x_{1} $

Answer:

Intercept means when the independent value is 0, the dependent value is the value of the intercept i.e with a nation with zero(0) gdp will have corruption rate of -1.272

Slope means that when the independent value increases by 1 amount, the dependent value increases by the amount Slope*1.


**2.3 Interpret the *substantive relevance* of the results. To do so, compare the predicted level of corruption in countries that are at the 25th and 75th percentile of the GDP per capita range. Would you describe the effect as large or small?**

```{r Exercise 2_3}

#@Group

```

Answer:


**2.4 Make a scatterplot of the corruption variable versus GDP per capita. Make an informed decision regarding the axes selection. Label the points with country names or abbreviations and add the regression line.**

```{r Exercise 2_4}

# I think according to the linear model there is a gradual increase in the corruption as gdp increases.

twocols <- viridis(2, alpha = c(0.5, 1))# only first color transparent

ols_bi <- function(y, x) {
  cov <- sum((x - mean(x)) * (y - mean(y)))
  var <- sum((x - mean(x))^2)
  b1 <- cov / var
  b0 <- mean(y) - b1 * mean(x)
  cat("Congrats, you just wrote your first estimator!\n \n intercept \t slope \n", b0, "\t", b1)
  return(c(b0, b1))
}
coef_hand <- ols_bi(y = df$cpi, x = df$gdp)

plot(x = df$gdp, 
     y = df$cpi,
     bty = "n",
     las = 1,
     pch = 19, 
     col = twocols[1], 
     cex = 1,
     ylim = c(0, 25),
     ylab = "Corruption in CPI",
     xlab = "GDP per capita",
     main = "@Group? Does the GDP influences the CPI index?)"
)
abline(a = coef_hand[1], # a is the intercept of the line
       b = coef_hand[2], # b is the slope of the line
       col = twocols[2],
       lwd = 2)

```

Answer:

There is a correlation between GDP and CPI. However, there are more countries with lower GDP compared to those countries with high GDP (right-screwed). In addition, there are many outliers in the data set that are difficult to explain with the model. Moreover, there is a large amount of unexplained variance in the model (RSS).

# @Team right?

**2.5 Imagine a fellow student seeks your advice. They generated the labeled scatterplot from 2.4 and are unsure whether they should keep the labels or not. What is your advice and why?**

Answer:

The person could add a logarithmic scale on the X axis. This would be useful because the data is not evenly distributed. There are many countries in the low GDP range and few in the high GDP range.

**2.6 What countries are unusually corrupt and lacking in corruption given their level of GDP per capita? Study the residual values of corruption (i.e., the values that cannot be explained or predicted by using information about GDP per capita). Include a residual plot, labeling any potentially interesting outliers.**

```{r Exercise 2_6}
ols_bi <- function(y, x) {
  cov <- sum((x - mean(x)) * (y - mean(y)))
  var <- sum((x - mean(x))^2)
  b1 <- cov / var
  b0 <- mean(y) - b1 * mean(x)
  return(c(b0, b1))
}

predict_bi <- function(x, intercept, slope) {
  intercept + slope * x
}

coef_hand <- ols_bi(y = df$cpi, x = df$gdp)
```


```{r}
cpi_hat <- predict_bi(
  x = df$gdp,
  intercept = coef_hand[1],
  slope = coef_hand[2]
)
df$residuals <- df$cpi - cpi_hat


```


```{r}
plot(x = df$gdp, 
     y = df$residuals,
     bty = "n",
     las = 1,
     main = "Residual Plot for GDP influence on CPI index",
     ylab = "Residuals",
     xlab = "GDP per capita",
     pch = 19,
     col = twocols[1])
abline(h = 0,
       col = twocols[2],
       lwd = 2)
grid() # add grid

largest_positive = head(df[order(df$residuals, decreasing= T),], n = 1)$cname
largest_negative = head(df[order(df$residuals, decreasing= F),], n = 1)$cname
# Label the largest positive outlier
text(x = df$gdp[residuals(linear_model_corruption) == max(residuals(linear_model_corruption))], 
     y = max(residuals(linear_model_corruption)), 
     labels = largest_positive,
     cex = 0.6,
     pos = 2)

# Label the largest negative outlier
text(x = df$gdp[residuals(linear_model_corruption) == min(residuals(linear_model_corruption))], 
     y = min(residuals(linear_model_corruption)), 
     labels = largest_negative,
     cex = 0.6,
     pos = 2)

```


Answer: According to the plot above we conclude that the outliers St Lucia and Equatorial Guinea can not be explained by the model. Therefore there might be other important influential factors. 

**2.7 Interpret the goodness-of-fit of the model.**

```{r Exercise 2_7}

TSS_hand <- sum((df$cpi - mean(df$cpi))^2)
ESS_hand <- sum((cpi_hat - mean(df$cpi))^2)
RSS_hand <- TSS_hand - ESS_hand

r_squared <- ESS_hand / TSS_hand





cat("TSS_hand: ", TSS_hand, "\nESS_hand: ", ESS_hand, "\nRSS_hand: ",RSS_hand, "\nr_squared: ", r_squared)


```

Answer: In general the goodness-of-fit measure ranges between 0 and 1. As our r_squared value is within the range, it can be said that the model is goodness-of-fit. With an R2 of 0.6 it means that 60% of the data fit can be explained by the model or the corresponding independent variable. 


There is a strong correlation between GDP and the level of corruption in a country. Furthermore, there is no clear correlation in the residual plot. 
However, other factors could also play a role, such as the form of government (democracy, autocracy, etc.).
Another factor could be the political orientation of the leading party.

**2.8 Adjusted R-squared by hand. Translate the following formula into R and compare your result to the output in the summary. How do you explain the difference between the $R^2$ and Adjusted $R^2$ for your model?**

$$Adj.R^2=1 - (1 - R^2)\frac{n-1}{n-k-1}$$

```{r Exercise 2_8}

n <- length(df$cname)
k <- 1
r_squared_adj <- 1 - ((1 - r_squared) * (n - 1)/(n - k - 1))
r_squared_adj

```
Answer:

The adjusted $Adj.R^2$ takes into account how many variables are used for the prediction. Therefore the $Adj.R^2$ does not get lower with more independend variables compared to the normal $R^2$


# R code

<!-- The chunk below will print out the code from all the chunks in the document, even if you chose to hide chunks in the main text with `echo=FALSE` chunk option or `include=FALSE` option. You do not need to put any code in this chunk manually: it will gather code from other chunks automatically. -->

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

# R code

<!-- The chunk below will print out the code from all the chunks in the document, even if you chose to hide chunks in the main text with `echo=FALSE` chunk option or `include=FALSE` option. You do not need to put any code in this chunk manually: it will gather code from other chunks automatically. -->

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```


